{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# from cuml import RandomForestClassifier as cuRF\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAG_REGEXP = \"#\\w+\"\n",
    "LINK_REGEXP = r\"https?://[\\w./-]+\"\n",
    "HANDLE_REGEXP = \"@\\S+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location(row, location_col='location'):\n",
    "    if row[location_col] is np.nan or not row[location_col].strip().replace(\"\\'|#|@\", \"\"):\n",
    "        return \"N/A\"\n",
    "    ret_val = re.sub(\"\\W+\", ' ', row[location_col].lower())\n",
    "    ret_val = re.sub(\"[\\s\\d]+\", ' ', ret_val.strip()).strip()\n",
    "    if len(ret_val) == 0:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return ret_val\n",
    "\n",
    "def lemmatize_sentence(text, tokenizer, lemmatizer):\n",
    "    tokens_pos_tag = pos_tag(tokenizer.tokenize(text))\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    tokens_wordnet = [\n",
    "        (word, tag_dict.get(pos[0], wordnet.NOUN)) for word, pos in tokens_pos_tag\n",
    "    ]\n",
    "    lem_tokens = [lemmatizer.lemmatize(word, pos) for word, pos in tokens_wordnet]\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, tokenizer, lemmatizer, text_col=\"text\", keyword_col='keyword'):\n",
    "    ret_df = df.copy()\n",
    "    return ret_df.assign(\n",
    "        keyword=lambda x: ret_df[keyword_col].fillna(\"N/A\"),\n",
    "        hashtag_in_text=ret_df[text_col].str.contains(HASHTAG_REGEXP),\n",
    "        hashtags=ret_df[text_col].apply(lambda x: re.findall(HASHTAG_REGEXP, x)),\n",
    "        link_in_text=ret_df[text_col].str.contains(LINK_REGEXP),\n",
    "        links=ret_df[text_col].apply(lambda x: re.findall(LINK_REGEXP, x)),\n",
    "        handle_in_text=ret_df[text_col].str.contains(HANDLE_REGEXP),\n",
    "        handles=ret_df[text_col].apply(lambda x: re.findall(HANDLE_REGEXP, x)),\n",
    "        location=ret_df.apply(clean_location, location_col=\"location\", axis=1),\n",
    "        tokens=ret_df[text_col].apply(tokenizer.tokenize),\n",
    "        lem_tokens=ret_df[text_col].apply(lemmatize_sentence, tokenizer=tokenizer,\n",
    "                                          lemmatizer=lemmatizer),\n",
    "        lem_text=lambda x: x[\"lem_tokens\"].apply(\" \".join),\n",
    "    )\n",
    "\n",
    "\n",
    "def reduce_locations(df, location_col=\"location\", id_col=\"id\"):\n",
    "    # Handle location categories that only appear a single time by replacing them with N/A\n",
    "    ret_df = df.copy()\n",
    "\n",
    "    single_location_list = (\n",
    "        ret_df.groupby(location_col, as_index=False)\n",
    "        .agg({id_col: \"count\"})\n",
    "        .rename(columns={id_col: \"row_count\"})\n",
    "        .query(\"row_count == 1\")[location_col]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    return ret_df.assign(\n",
    "        location=ret_df[location_col].where(\n",
    "            ~ret_df[location_col].isin(single_location_list), other=\"N/A\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# train_df = train_df.assign(location = train_df[\"location_orig\"].where(\n",
    "#     ~train_df[\"location_orig\"].isin(single_location_list), other=\"N/A\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "train_df = (pd.read_csv(\"./train.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger, StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to take the sentence and output the lemmatized list\n",
    "    - Input is text\n",
    "    - Next tokenize and get pos_tag\n",
    "    - Use the pos_tage with token as input to the lemmatizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postag = StanfordPOSTagger('english-bidirectional-distsim.tagger',\n",
    "                           '/home/sjustice/My_code/stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "nert = StanfordNERTagger('/home/sjustice/My_code/stanford-tagger-4.0.0/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import download\n",
    "# download('averaged_perceptron_tagger')\n",
    "# download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a lot of location categories that only appear once\n",
    "    - 2818 locations only appear once\n",
    "    - Some of them are in lowercase\n",
    "    - Convert the location to lower and remove punctuation\n",
    "    - Maybe remove those or ones with less than 5 occurances and replace them with None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace all locations that appear only once with N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform a train/test split and then use the stemmer and the vectorizer on the train set\n",
    "- Need to be able to align the features from the train set so that they are the same in the test set\n",
    "    - Only include the ones in the train set - No new features in the test set\n",
    "    \n",
    "- Perform lematization on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[\n",
    "        [\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]\n",
    "    ],\n",
    "    train_df[\"target\"],\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"target\"],\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_list =['keyword', 'location', 'hash_in_text', 'handle_in_text']\n",
    "\n",
    "# Use a column transformer instead of the FeatureUnion since it accomplishes the same thing without\n",
    "# needing a custom class\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"text_transform\",\n",
    "            ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat_feature_list),\n",
    "                                  (\"tfidf\", TfidfVectorizer(), 'lem_text')]),\n",
    "        ),\n",
    "#         (\"cudf_convert\", CudfConversion()),\n",
    "        (\"rf\", RandomForestClassifier(random_state=42, n_jobs=1) )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things to check for in the text\n",
    "    - @s - tweets directed at other users\n",
    "    - #s - hashtags\n",
    "    - retweets - urls\n",
    "    - Name entity\n",
    "    - Allcaps words??? - Remove them or keep them?\n",
    "    - Remove the ats, hashtags, and urls from the text before putting it through a tfidf vectorizer\n",
    "    - A lot of repeated location categories - Need to handle instances like 'Chicago, IL' and 'Chicago,IL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"rf__n_estimators\": range(3000, 10001, 500),\n",
    "    \"rf__max_depth\": [None, 10, 20, 40],\n",
    "    \"rf__min_samples_split\": [2, 4, 6],\n",
    "    \"rf__min_samples_leaf\": [1, 2, 3],\n",
    "    \"rf__max_features\": [\"auto\", \"log2\", 20, 40, 60, 100],\n",
    "    \"text_transform__tfidf__ngram_range\": [(1,1), (1,2), (1,3), (2,3)],\n",
    "    \"text_transform__tfidf__max_df\": [0.9, 0.95, 0.99],\n",
    "    \"text_transform__tfidf__min_df\": [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=30,\n",
    "                         scoring='f1', cv=3, random_state=42, verbose=2, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=7)]: Done  90 out of  90 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('text_transform',\n",
       "                                              ColumnTransformer(transformers=[('ohe',\n",
       "                                                                               OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                               ['keyword',\n",
       "                                                                                'location',\n",
       "                                                                                'hash_in_text',\n",
       "                                                                                'handle_in_text']),\n",
       "                                                                              ('tfidf',\n",
       "                                                                               TfidfVectorizer(),\n",
       "                                                                               'lem_text')])),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(n_jobs=1,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=30, n_jobs=7,\n",
       "                   param_distributions={'rf__max...0, 40],\n",
       "                                        'rf__max_features': ['auto', 'log2', 20,\n",
       "                                                             40, 60, 100],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 3],\n",
       "                                        'rf__min_samples_split': [2, 4, 6],\n",
       "                                        'rf__n_estimators': range(3000, 10001, 500),\n",
       "                                        'text_transform__tfidf__max_df': [0.9,\n",
       "                                                                          0.95,\n",
       "                                                                          0.99],\n",
       "                                        'text_transform__tfidf__min_df': [2, 5,\n",
       "                                                                          10],\n",
       "                                        'text_transform__tfidf__ngram_range': [(1,\n",
       "                                                                                1),\n",
       "                                                                               (1,\n",
       "                                                                                2),\n",
       "                                                                               (1,\n",
       "                                                                                3),\n",
       "                                                                               (2,\n",
       "                                                                                3)]},\n",
       "                   random_state=42, scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_transform__tfidf__ngram_range': (1, 1),\n",
       " 'text_transform__tfidf__min_df': 10,\n",
       " 'text_transform__tfidf__max_df': 0.99,\n",
       " 'rf__n_estimators': 7000,\n",
       " 'rf__min_samples_split': 4,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7210300429184551"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rf_model_with_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(rcv, f)\n",
    "\n",
    "# # and later you can load it\n",
    "# with open('filename.pkl', 'rb') as f:\n",
    "#     clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (pd.read_csv(\"./test.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>handle_in_text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6234</th>\n",
       "      <td>snowstorm</td>\n",
       "      <td>south usa</td>\n",
       "      <td>Sassy city girl country hunk stranded in Smoky...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Sassy city girl country hunk strand in Smoky M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>armageddon</td>\n",
       "      <td>worldwide</td>\n",
       "      <td>God's Kingdom (Heavenly Gov't) will rule over ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>God's Kingdom ( Heavenly Gov't ) will rule ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Mopheme and Bigstar Johnson are a problem in t...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Mopheme and Bigstar Johnson be a problem in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>whirlwind</td>\n",
       "      <td>N/A</td>\n",
       "      <td>@VixMeldrew sounds like a whirlwind life!</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@VixMeldrew sound like a whirlwind life !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>debris</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>Malaysia confirms plane debris washed up on Re...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Malaysia confirm plane debris wash up on Reuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword   location  \\\n",
       "6234       snowstorm  south usa   \n",
       "326       armageddon  worldwide   \n",
       "997   body%20bagging        N/A   \n",
       "7269       whirlwind        N/A   \n",
       "2189          debris    nigeria   \n",
       "\n",
       "                                                   text  hashtag_in_text  \\\n",
       "6234  Sassy city girl country hunk stranded in Smoky...             True   \n",
       "326   God's Kingdom (Heavenly Gov't) will rule over ...            False   \n",
       "997   Mopheme and Bigstar Johnson are a problem in t...             True   \n",
       "7269          @VixMeldrew sounds like a whirlwind life!            False   \n",
       "2189  Malaysia confirms plane debris washed up on Re...            False   \n",
       "\n",
       "      link_in_text  handle_in_text  \\\n",
       "6234          True           False   \n",
       "326           True           False   \n",
       "997          False           False   \n",
       "7269         False            True   \n",
       "2189          True           False   \n",
       "\n",
       "                                               lem_text  \n",
       "6234  Sassy city girl country hunk strand in Smoky M...  \n",
       "326   God's Kingdom ( Heavenly Gov't ) will rule ove...  \n",
       "997   Mopheme and Bigstar Johnson be a problem in th...  \n",
       "7269          @VixMeldrew sound like a whirlwind life !  \n",
       "2189  Malaysia confirm plane debris wash up on Reuni...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>handle_in_text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Just happen a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Heard about #earthquake be different city , st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>there be a forest fire at spot pond , geese be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Apocalypse light . #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Typhoon Soudelor kill 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  \\\n",
       "0     N/A      N/A                 Just happened a terrible car crash   \n",
       "1     N/A      N/A  Heard about #earthquake is different cities, s...   \n",
       "2     N/A      N/A  there is a forest fire at spot pond, geese are...   \n",
       "3     N/A      N/A           Apocalypse lighting. #Spokane #wildfires   \n",
       "4     N/A      N/A      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   hashtag_in_text  link_in_text  handle_in_text  \\\n",
       "0            False         False           False   \n",
       "1             True         False           False   \n",
       "2            False         False           False   \n",
       "3             True         False           False   \n",
       "4            False         False           False   \n",
       "\n",
       "                                            lem_text  \n",
       "0                   Just happen a terrible car crash  \n",
       "1  Heard about #earthquake be different city , st...  \n",
       "2  there be a forest fire at spot pond , geese be...  \n",
       "3             Apocalypse light . #Spokane #wildfires  \n",
       "4       Typhoon Soudelor kill 28 in China and Taiwan  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe.predict(test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].rename(columns={\"hashtag_in_text\": \"hash_in_text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(target=best_pipe.predict(test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].rename(columns={\"hashtag_in_text\": \"hash_in_text\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id', 'target']].to_csv(\"submission_0706.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
