{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml import RandomForestClassifier as cuRF\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(text):\n",
    "    return re.findall(\"(#\\w+)\", text)\n",
    "\n",
    "def extract_link(text):\n",
    "    return re.findall(r\"https?://[\\w./-]+\", text)\n",
    "\n",
    "def extract_handles(text):\n",
    "    return re.findall(\"@\\S+\", text)\n",
    "\n",
    "def remove_elements_from_text(row, text_col, remove_col_list):\n",
    "    text = row[text_col]\n",
    "    for cur_col in remove_col_list:\n",
    "        text = \" \".join([x for x in text.split() if x not in row[cur_col]])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location(row, location_col='location'):\n",
    "    if row[location_col] is np.nan or not row[location_col].strip().replace(\"\\'|#|@\", \"\"):\n",
    "        return \"N/A\"\n",
    "    ret_val = re.sub(\"\\W+\", ' ', row[location_col].lower())\n",
    "    ret_val = re.sub(\"[\\s\\d]+\", ' ', ret_val.strip()).strip()\n",
    "    if len(ret_val) == 0:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger, StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to take the sentence and output the lemmatized list\n",
    "    - Input is text\n",
    "    - Next tokenize and get pos_tag\n",
    "    - Use the pos_tage with token as input to the lemmatizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(text):\n",
    "    tokens_pos_tag = pos_tag(tknzr.tokenize(text))\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    tokens_wordnet = [\n",
    "        (word, tag_dict.get(pos[0], wordnet.NOUN)) for word, pos in tokens_pos_tag\n",
    "    ]\n",
    "    lem_tokens = [wnl.lemmatize(word, pos) for word, pos in tokens_wordnet]\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudfConversion(TransformerMixin):\n",
    "    def __init__(self):\n",
    "#         self.cudf = cudf.DataFrame()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         self.df = cudf.from_pandas(pd.DataFrame.sparse.from_spmatrix(data=X))\n",
    "#         return df\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return pd.DataFrame(data=X.todense()).astype('float32')\n",
    "#         self.cudf = cudf.from_pandas(pd.DataFrame.sparse.from_spmatrix(data=X)).astype('float32')\n",
    "#         return self.cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the english-bidirectional-distsim.tagger file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-9a205fc481ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m postag = StanfordPOSTagger('english-bidirectional-distsim.tagger',\n\u001b[0;32m----> 2\u001b[0;31m                            '/home/sjustice/My_code/stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger')\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         self._stanford_model = find_file(\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STANFORD_MODELS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_file\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[1;32m    644\u001b[0m ):\n\u001b[1;32m    645\u001b[0m     return next(\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mfind_file_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the english-bidirectional-distsim.tagger file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "postag = StanfordPOSTagger('english-bidirectional-distsim.tagger',\n",
    "                           '/home/sjustice/My_code/stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nert = StanfordNERTagger('/home/sjustice/My_code/stanford-tagger-4.0.0/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sjustice/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/sjustice/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('averaged_perceptron_tagger')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\").assign(\n",
    "    keyword=lambda x: x['keyword'].fillna(\"N/A\"),\n",
    "    hash_in_text=lambda x: x[\"text\"].str.contains(\"#\\w+\"),\n",
    "    hashtags=lambda x: x[\"text\"].apply(extract_hashtags),\n",
    "    link_in_text=lambda x: x[\"text\"].str.contains(\"http\"),\n",
    "    links=lambda x: x[\"text\"].apply(extract_link),\n",
    "    handle_in_text=lambda x: x['text'].str.contains(\"@\\S+\"),\n",
    "    handles=lambda x: x['text'].apply(extract_handles),\n",
    "    reduced_text=lambda x: x.apply(remove_elements_from_text, text_col='text',\n",
    "                                   remove_col_list=['hashtags', 'links', 'handles'], axis=1),\n",
    "    location_orig=lambda x: x.apply(clean_location, axis=1),\n",
    "    tokens=lambda x: x['text'].apply(tknzr.tokenize),\n",
    "    lem_tokens=lambda x: x['text'].apply(lemmatize_sentence),\n",
    "    lem_text=lambda x: x['lem_tokens'].apply(\" \".join)\n",
    ")\n",
    "\n",
    "single_location_list = train_df.groupby(\"location_orig\", as_index=False).agg({\"id\": \"count\"}).rename(\n",
    "    columns={\"id\": \"count\"}\n",
    ").query(\"count == 1\")[\"location_orig\"].tolist()\n",
    "\n",
    "train_df = train_df.assign(location = train_df[\"location_orig\"].where(\n",
    "    ~train_df[\"location_orig\"].isin(single_location_list), other=\"N/A\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a lot of location categories that only appear once\n",
    "    - 2818 locations only appear once\n",
    "    - Some of them are in lowercase\n",
    "    - Convert the location to lower and remove punctuation\n",
    "    - Maybe remove those or ones with less than 5 occurances and replace them with None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace all locations that appear only once with N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform a train/test split and then use the stemmer and the vectorizer on the train set\n",
    "- Need to be able to align the features from the train set so that they are the same in the test set\n",
    "    - Only include the ones in the train set - No new features in the test set\n",
    "    \n",
    "- Perform lematization on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a column transformer instead of the FeatureUnion since it accomplishes the same thing without\n",
    "# needing a custom class\n",
    "# Will need a custom class for the cudf conversion though (I think??)\n",
    "column_trans = ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore', dtype=np.int32), cat_feature_list),\n",
    "                                  (\"tfidf\", TfidfVectorizer(dtype=np.float32), 'lem_text')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[\n",
    "        [\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hash_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]\n",
    "    ],\n",
    "    train_df[\"target\"],\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"target\"],\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_list =['keyword', 'location', 'hash_in_text', 'handle_in_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"text_transform\",\n",
    "            ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat_feature_list),\n",
    "                                  (\"tfidf\", TfidfVectorizer(), 'lem_text')]),\n",
    "        ),\n",
    "#         (\"cudf_convert\", CudfConversion()),\n",
    "        (\"rf\", RandomForestClassifier(random_state=42, n_jobs=1) )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things to check for in the text\n",
    "    - @s - tweets directed at other users\n",
    "    - #s - hashtags\n",
    "    - retweets - urls\n",
    "    - Name entity\n",
    "    - Allcaps words??? - Remove them or keep them?\n",
    "    - Remove the ats, hashtags, and urls from the text before putting it through a tfidf vectorizer\n",
    "    - A lot of repeated location categories - Need to handle instances like 'Chicago, IL' and 'Chicago,IL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"rf__n_estimators\": range(3000, 10001, 500),\n",
    "    \"rf__max_depth\": [None, 10, 20, 40],\n",
    "    \"rf__min_samples_split\": [2, 4, 6],\n",
    "    \"rf__min_samples_leaf\": [1, 2, 3],\n",
    "    \"rf__max_features\": [\"auto\", \"log2\", 20, 40, 60, 100],\n",
    "    \"text_transform__tfidf__ngram_range\": [(1,1), (1,2), (1,3), (2,3)],\n",
    "    \"text_transform__tfidf__max_df\": [0.9, 0.95, 0.99],\n",
    "    \"text_transform__tfidf__min_df\": [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=100,\n",
    "                         scoring='f1', cv=3, random_state=42, verbose=2, n_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:   41.1s\n",
      "/home/sjustice/anaconda3/envs/gpu/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=15)]: Done 132 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=15)]: Done 300 out of 300 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('text_transform',\n",
       "                                              ColumnTransformer(transformers=[('ohe',\n",
       "                                                                               OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                               ['keyword',\n",
       "                                                                                'location',\n",
       "                                                                                'hash_in_text',\n",
       "                                                                                'handle_in_text']),\n",
       "                                                                              ('tfidf',\n",
       "                                                                               TfidfVectorizer(),\n",
       "                                                                               'lem_text')])),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(n_jobs=1,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=100, n_jobs=15,\n",
       "                   param_distributions={'rf__m...0, 40],\n",
       "                                        'rf__max_features': ['auto', 'log2', 20,\n",
       "                                                             40, 60, 100],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 3],\n",
       "                                        'rf__min_samples_split': [2, 4, 6],\n",
       "                                        'rf__n_estimators': range(3000, 10001, 500),\n",
       "                                        'text_transform__tfidf__max_df': [0.9,\n",
       "                                                                          0.95,\n",
       "                                                                          0.99],\n",
       "                                        'text_transform__tfidf__min_df': [2, 5,\n",
       "                                                                          10],\n",
       "                                        'text_transform__tfidf__ngram_range': [(1,\n",
       "                                                                                1),\n",
       "                                                                               (1,\n",
       "                                                                                2),\n",
       "                                                                               (1,\n",
       "                                                                                3),\n",
       "                                                                               (2,\n",
       "                                                                                3)]},\n",
       "                   random_state=42, scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_transform__tfidf__ngram_range': (1, 1),\n",
       " 'text_transform__tfidf__min_df': 10,\n",
       " 'text_transform__tfidf__max_df': 0.9,\n",
       " 'rf__n_estimators': 9500,\n",
       " 'rf__min_samples_split': 4,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': None}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7003852673755585"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import RandomForestClassifier as cuRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"text_transform\",\n",
    "            ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore', dtype=np.int32), cat_feature_list),\n",
    "                              (\"tfidf\", TfidfVectorizer(dtype=np.float32), 'lem_text')]),\n",
    "        ),\n",
    "        (\"cudf_convert\", CudfConversion()),\n",
    "        (\"curf\", cuRF() )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_param_grid = {\n",
    "#     \"curf__convert_dtype\": [True],\n",
    "    \"curf__n_estimators\": range(3000, 10001, 500),\n",
    "    \"curf__max_depth\": [10, 20, 40],\n",
    "#     \"curf__min_samples_split\": [2, 4, 6],\n",
    "    \"curf__min_rows_per_node\": [1, 2, 3],\n",
    "    \"curf__max_features\": [\"auto\", \"log2\", 20, 40, 60, 100],\n",
    "    \"text_transform__tfidf__ngram_range\": [(1,1), (1,2), (1,3), (2,3)],\n",
    "    \"text_transform__tfidf__max_df\": [0.9, 0.95, 0.99],\n",
    "    \"text_transform__tfidf__min_df\": [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_rcv = RandomizedSearchCV(estimator=cuda_pipe, param_distributions=cuda_param_grid, n_iter=100,\n",
    "                              scoring='f1', cv=3, random_state=42, verbose=2, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] text_transform__tfidf__ngram_range=(1, 3), text_transform__tfidf__min_df=2, text_transform__tfidf__max_df=0.9, curf__n_estimators=9000, curf__min_rows_per_node=2, curf__max_features=20, curf__max_depth=40 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "cuda_rcv.fit(X_train, y_train, curf__convert_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7204523306750437"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_gcv = GridSearchCV(estimator=cuda_pipe, param_grid=cuda_param_grid, scoring='f1', cv=3, verbose=2, n_jobs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu]",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
