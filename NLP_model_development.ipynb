{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# from cuml import RandomForestClassifier as cuRF\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAG_REGEXP = \"#\\w+\"\n",
    "LINK_REGEXP = r\"https?://[\\w./-]+\"\n",
    "HANDLE_REGEXP = \"@\\S+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location(row, location_col='location'):\n",
    "    if row[location_col] is np.nan or not row[location_col].strip().replace(\"\\'|#|@\", \"\"):\n",
    "        return \"N/A\"\n",
    "    ret_val = re.sub(\"\\W+\", ' ', row[location_col].lower())\n",
    "    ret_val = re.sub(\"[\\s\\d]+\", ' ', ret_val.strip()).strip()\n",
    "    if len(ret_val) == 0:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return ret_val\n",
    "\n",
    "def lemmatize_sentence(text, tokenizer, lemmatizer):\n",
    "    tokens_pos_tag = pos_tag(tokenizer.tokenize(text))\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    tokens_wordnet = [\n",
    "        (word, tag_dict.get(pos[0], wordnet.NOUN)) for word, pos in tokens_pos_tag\n",
    "    ]\n",
    "    lem_tokens = [lemmatizer.lemmatize(word, pos) for word, pos in tokens_wordnet]\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, tokenizer, lemmatizer, text_col=\"text\", keyword_col='keyword'):\n",
    "    ret_df = df.copy()\n",
    "    return ret_df.assign(\n",
    "        keyword=lambda x: ret_df[keyword_col].fillna(\"N/A\"),\n",
    "        hashtag_in_text=ret_df[text_col].str.contains(HASHTAG_REGEXP),\n",
    "        hashtags=ret_df[text_col].apply(lambda x: re.findall(HASHTAG_REGEXP, x)),\n",
    "        link_in_text=ret_df[text_col].str.contains(LINK_REGEXP),\n",
    "        links=ret_df[text_col].apply(lambda x: re.findall(LINK_REGEXP, x)),\n",
    "        handle_in_text=ret_df[text_col].str.contains(HANDLE_REGEXP),\n",
    "        handles=ret_df[text_col].apply(lambda x: re.findall(HANDLE_REGEXP, x)),\n",
    "        location=ret_df.apply(clean_location, location_col=\"location\", axis=1),\n",
    "        tokens=ret_df[text_col].apply(tokenizer.tokenize),\n",
    "        lem_tokens=ret_df[text_col].apply(lemmatize_sentence, tokenizer=tokenizer,\n",
    "                                          lemmatizer=lemmatizer),\n",
    "        lem_text=lambda x: x[\"lem_tokens\"].apply(\" \".join),\n",
    "    )\n",
    "\n",
    "\n",
    "def reduce_locations(df, location_col=\"location\", id_col=\"id\"):\n",
    "    # Handle location categories that only appear a single time by replacing them with N/A\n",
    "    ret_df = df.copy()\n",
    "\n",
    "    single_location_list = (\n",
    "        ret_df.groupby(location_col, as_index=False)\n",
    "        .agg({id_col: \"count\"})\n",
    "        .rename(columns={id_col: \"row_count\"})\n",
    "        .query(\"row_count == 1\")[location_col]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    return ret_df.assign(\n",
    "        location=ret_df[location_col].where(\n",
    "            ~ret_df[location_col].isin(single_location_list), other=\"N/A\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# train_df = train_df.assign(location = train_df[\"location_orig\"].where(\n",
    "#     ~train_df[\"location_orig\"].isin(single_location_list), other=\"N/A\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "train_df = (pd.read_csv(\"./train.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger, StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to take the sentence and output the lemmatized list\n",
    "    - Input is text\n",
    "    - Next tokenize and get pos_tag\n",
    "    - Use the pos_tage with token as input to the lemmatizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postag = StanfordPOSTagger('english-bidirectional-distsim.tagger',\n",
    "                           '/home/sjustice/My_code/stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "nert = StanfordNERTagger('/home/sjustice/My_code/stanford-tagger-4.0.0/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import download\n",
    "# download('averaged_perceptron_tagger')\n",
    "# download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a lot of location categories that only appear once\n",
    "    - 2818 locations only appear once\n",
    "    - Some of them are in lowercase\n",
    "    - Convert the location to lower and remove punctuation\n",
    "    - Maybe remove those or ones with less than 5 occurances and replace them with None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace all locations that appear only once with N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform a train/test split and then use the stemmer and the vectorizer on the train set\n",
    "- Need to be able to align the features from the train set so that they are the same in the test set\n",
    "    - Only include the ones in the train set - No new features in the test set\n",
    "    \n",
    "- Perform lematization on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[\n",
    "        [\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hash_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]\n",
    "    ],\n",
    "    train_df[\"target\"],\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"target\"],\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_list =['keyword', 'location', 'hash_in_text', 'handle_in_text']\n",
    "\n",
    "# Use a column transformer instead of the FeatureUnion since it accomplishes the same thing without\n",
    "# needing a custom class\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"text_transform\",\n",
    "            ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat_feature_list),\n",
    "                                  (\"tfidf\", TfidfVectorizer(), 'lem_text')]),\n",
    "        ),\n",
    "#         (\"cudf_convert\", CudfConversion()),\n",
    "        (\"rf\", RandomForestClassifier(random_state=42, n_jobs=1) )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things to check for in the text\n",
    "    - @s - tweets directed at other users\n",
    "    - #s - hashtags\n",
    "    - retweets - urls\n",
    "    - Name entity\n",
    "    - Allcaps words??? - Remove them or keep them?\n",
    "    - Remove the ats, hashtags, and urls from the text before putting it through a tfidf vectorizer\n",
    "    - A lot of repeated location categories - Need to handle instances like 'Chicago, IL' and 'Chicago,IL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"rf__n_estimators\": range(3000, 10001, 500),\n",
    "    \"rf__max_depth\": [None, 10, 20, 40],\n",
    "    \"rf__min_samples_split\": [2, 4, 6],\n",
    "    \"rf__min_samples_leaf\": [1, 2, 3],\n",
    "    \"rf__max_features\": [\"auto\", \"log2\", 20, 40, 60, 100],\n",
    "    \"text_transform__tfidf__ngram_range\": [(1,1), (1,2), (1,3), (2,3)],\n",
    "    \"text_transform__tfidf__max_df\": [0.9, 0.95, 0.99],\n",
    "    \"text_transform__tfidf__min_df\": [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=30,\n",
    "                         scoring='f1', cv=3, random_state=42, verbose=2, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=7)]: Done  90 out of  90 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('text_transform',\n",
       "                                              ColumnTransformer(transformers=[('ohe',\n",
       "                                                                               OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                               ['keyword',\n",
       "                                                                                'location',\n",
       "                                                                                'hash_in_text',\n",
       "                                                                                'handle_in_text']),\n",
       "                                                                              ('tfidf',\n",
       "                                                                               TfidfVectorizer(),\n",
       "                                                                               'lem_text')])),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(n_jobs=1,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=30, n_jobs=7,\n",
       "                   param_distributions={'rf__max...0, 40],\n",
       "                                        'rf__max_features': ['auto', 'log2', 20,\n",
       "                                                             40, 60, 100],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 3],\n",
       "                                        'rf__min_samples_split': [2, 4, 6],\n",
       "                                        'rf__n_estimators': range(3000, 10001, 500),\n",
       "                                        'text_transform__tfidf__max_df': [0.9,\n",
       "                                                                          0.95,\n",
       "                                                                          0.99],\n",
       "                                        'text_transform__tfidf__min_df': [2, 5,\n",
       "                                                                          10],\n",
       "                                        'text_transform__tfidf__ngram_range': [(1,\n",
       "                                                                                1),\n",
       "                                                                               (1,\n",
       "                                                                                2),\n",
       "                                                                               (1,\n",
       "                                                                                3),\n",
       "                                                                               (2,\n",
       "                                                                                3)]},\n",
       "                   random_state=42, scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_transform__tfidf__ngram_range': (1, 1),\n",
       " 'text_transform__tfidf__min_df': 10,\n",
       " 'text_transform__tfidf__max_df': 0.99,\n",
       " 'rf__n_estimators': 7000,\n",
       " 'rf__min_samples_split': 4,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7210300429184551"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rf_model_with_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(rcv, f)\n",
    "\n",
    "# # and later you can load it\n",
    "# with open('filename.pkl', 'rb') as f:\n",
    "#     clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (pd.read_csv(\"./test.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column ordering must be equal for fit and for transform when using the remainder keyword",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-625ce661b4d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    596\u001b[0m             if (n_cols_transform >= n_cols_fit and\n\u001b[1;32m    597\u001b[0m                     any(X.columns[:n_cols_fit] != self._df_columns)):\n\u001b[0;32m--> 598\u001b[0;31m                 raise ValueError('Column ordering must be equal for fit '\n\u001b[0m\u001b[1;32m    599\u001b[0m                                  \u001b[0;34m'and for transform when using the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                                  'remainder keyword')\n",
      "\u001b[0;31mValueError\u001b[0m: Column ordering must be equal for fit and for transform when using the remainder keyword"
     ]
    }
   ],
   "source": [
    "best_pipe.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
