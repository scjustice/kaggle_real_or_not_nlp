{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cudf\n",
    "# from cuml import RandomForestClassifier as cuRF\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "import string\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAG_REGEXP = \"#\\w+\"\n",
    "LINK_REGEXP = r\"https?://[\\w./-]+\"\n",
    "HANDLE_REGEXP = \"@\\S+\"\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location(row, location_col='location'):\n",
    "    if row[location_col] is np.nan or not row[location_col].strip().replace(\"\\'|#|@\", \"\"):\n",
    "        return \"N/A\"\n",
    "    ret_val = re.sub(\"\\W+\", ' ', row[location_col].lower())\n",
    "    ret_val = re.sub(\"[\\s\\d]+\", ' ', ret_val.strip()).strip()\n",
    "    if len(ret_val) == 0:\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        return ret_val\n",
    "\n",
    "def lemmatize_sentence(text, tokenizer, lemmatizer):\n",
    "    tokens_pos_tag = pos_tag(tokenizer.tokenize(text))\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "    tokens_wordnet = [\n",
    "        (word, tag_dict.get(pos[0], wordnet.NOUN)) for word, pos in tokens_pos_tag\n",
    "    ]\n",
    "    lem_tokens = [lemmatizer.lemmatize(word, pos) for word, pos in tokens_wordnet]\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, tokenizer, lemmatizer, text_col=\"text\", keyword_col=\"keyword\"):\n",
    "    ret_df = df.copy()\n",
    "    return ret_df.assign(\n",
    "        keyword=lambda x: ret_df[keyword_col].fillna(\"N/A\"),\n",
    "        hashtag_in_text=ret_df[text_col].str.contains(HASHTAG_REGEXP),\n",
    "        hashtags=ret_df[text_col].apply(lambda x: re.findall(HASHTAG_REGEXP, x)),\n",
    "        link_in_text=ret_df[text_col].str.contains(LINK_REGEXP),\n",
    "        links=ret_df[text_col].apply(lambda x: re.findall(LINK_REGEXP, x)),\n",
    "        mention_in_text=ret_df[text_col].str.contains(HANDLE_REGEXP),\n",
    "        mentions=ret_df[text_col].apply(lambda x: re.findall(HANDLE_REGEXP, x)),\n",
    "        location=ret_df.apply(clean_location, location_col=\"location\", axis=1),\n",
    "        tokens=ret_df[text_col].apply(tokenizer.tokenize),\n",
    "        lem_tokens=ret_df[text_col].apply(\n",
    "            lemmatize_sentence, tokenizer=tokenizer, lemmatizer=lemmatizer\n",
    "        ),\n",
    "        lem_text=lambda x: x[\"lem_tokens\"].apply(\" \".join),\n",
    "        word_count=lambda x: x[\"tokens\"].apply(len),\n",
    "        unique_tokens=lambda x: x[\"tokens\"].apply(set),\n",
    "        unique_word_count=lambda x: x[\"unique_tokens\"].apply(len),\n",
    "        stop_word_count=lambda x: x[\"tokens\"].apply(\n",
    "            lambda y: len([token for token in y if token in STOPWORDS])\n",
    "        ),\n",
    "        link_count=lambda x: x[\"links\"].apply(len),\n",
    "        hashtag_count=lambda x: x[\"hashtags\"].apply(len),\n",
    "        mention_count=lambda x: x[\"mentions\"].apply(len),\n",
    "        char_count=lambda x: x[\"tokens\"].apply(\n",
    "            lambda y: np.sum([len(token) for token in y])\n",
    "        ),\n",
    "        mean_word_length=lambda x: x[\"char_count\"] / x[\"word_count\"],\n",
    "        punctuation_count=lambda x: x[\"tokens\"].apply(\n",
    "            lambda y: len([token for token in y if token in string.punctuation])\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def reduce_locations(df, location_col=\"location\", id_col=\"id\"):\n",
    "    # Handle location categories that only appear a single time by replacing them with N/A\n",
    "    ret_df = df.copy()\n",
    "\n",
    "    single_location_list = (\n",
    "        ret_df.groupby(location_col, as_index=False)\n",
    "        .agg({id_col: \"count\"})\n",
    "        .rename(columns={id_col: \"row_count\"})\n",
    "        .query(\"row_count == 1\")[location_col]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    return ret_df.assign(\n",
    "        location=ret_df[location_col].where(\n",
    "            ~ret_df[location_col].isin(single_location_list), other=\"N/A\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# train_df = train_df.assign(location = train_df[\"location_orig\"].where(\n",
    "#     ~train_df[\"location_orig\"].isin(single_location_list), other=\"N/A\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "train_df = (pd.read_csv(\"./train.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to Add\n",
    "- word_count number of words in text\n",
    "- unique_word_count number of unique words in text\n",
    "- stop_word_count number of stop words in text\n",
    "- url_count number of urls in text\n",
    "- mean_word_length average character count in words\n",
    "- char_count number of characters in text\n",
    "- punctuation_count number of punctuations in text\n",
    "- hashtag_count number of hashtags (#) in text\n",
    "- mention_count number of mentions (@) in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>links</th>\n",
       "      <th>mention_in_text</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>{of, Forgive, this, May, the, Deeds, Our, #ear...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>{Sask, Canada, Forest, La, near, ., Ronge, fire}</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>{expected, officers, ., to, ', in, residents, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>{orders, receive, #wildfires, California, in, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>{school, as, sent, into, this, #wildfires, fro...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     N/A      N/A  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     N/A      N/A             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     N/A      N/A  All residents asked to 'shelter in place' are ...   \n",
       "3   6     N/A      N/A  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     N/A      N/A  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  hashtag_in_text               hashtags  link_in_text links  \\\n",
       "0       1             True          [#earthquake]         False    []   \n",
       "1       1            False                     []         False    []   \n",
       "2       1            False                     []         False    []   \n",
       "3       1             True           [#wildfires]         False    []   \n",
       "4       1             True  [#Alaska, #wildfires]         False    []   \n",
       "\n",
       "   mention_in_text  ... word_count  \\\n",
       "0            False  ...         13   \n",
       "1            False  ...          8   \n",
       "2            False  ...         25   \n",
       "3            False  ...          8   \n",
       "4            False  ...         16   \n",
       "\n",
       "                                       unique_tokens unique_word_count  \\\n",
       "0  {of, Forgive, this, May, the, Deeds, Our, #ear...                13   \n",
       "1   {Sask, Canada, Forest, La, near, ., Ronge, fire}                 8   \n",
       "2  {expected, officers, ., to, ', in, residents, ...                20   \n",
       "3  {orders, receive, #wildfires, California, in, ...                 8   \n",
       "4  {school, as, sent, into, this, #wildfires, fro...                15   \n",
       "\n",
       "  stop_word_count  link_count hashtag_count  mention_count  char_count  \\\n",
       "0               5           0             1              0          57   \n",
       "1               0           0             0              0          32   \n",
       "2               9           0             0              0         112   \n",
       "3               1           0             1              0          57   \n",
       "4               6           0             2              0          72   \n",
       "\n",
       "   mean_word_length  punctuation_count  \n",
       "0          4.384615                  0  \n",
       "1          4.000000                  1  \n",
       "2          4.480000                  3  \n",
       "3          7.125000                  0  \n",
       "4          4.500000                  0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       3\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7608    0\n",
       "7609    2\n",
       "7610    5\n",
       "7611    2\n",
       "7612    2\n",
       "Name: tokens, Length: 7613, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tokens'].apply(lambda x: len([token for token in x if token in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>links</th>\n",
       "      <th>mention_in_text</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>{of, Forgive, this, May, the, Deeds, Our, #ear...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>{Sask, Canada, Forest, La, near, ., Ronge, fire}</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>{expected, officers, ., to, ', in, residents, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>4.480000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>{orders, receive, #wildfires, California, in, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>{school, as, sent, into, this, #wildfires, fro...</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[http://t.co/STfMbbZFB5]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>{giant, into, homes, collapse, Two, nearby, br...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>6.636364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>{of, state, even, troubling, fires, California...</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[http://t.co/zDtoyd8EbJ]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>{Volcano, of, [, 5km, M1, UTC, ., 94, ], http:...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>{E-bike, injuries, ., in, an, with, collided, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[http://t.co/YmY4rSkQ3d]</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>{-, Latest, News, Northern, Wildfire, Californ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     N/A      N/A   \n",
       "1         4     N/A      N/A   \n",
       "2         5     N/A      N/A   \n",
       "3         6     N/A      N/A   \n",
       "4         7     N/A      N/A   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     N/A      N/A   \n",
       "7609  10870     N/A      N/A   \n",
       "7610  10871     N/A      N/A   \n",
       "7611  10872     N/A      N/A   \n",
       "7612  10873     N/A      N/A   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "      hashtag_in_text               hashtags  link_in_text  \\\n",
       "0                True          [#earthquake]         False   \n",
       "1               False                     []         False   \n",
       "2               False                     []         False   \n",
       "3                True           [#wildfires]         False   \n",
       "4                True  [#Alaska, #wildfires]         False   \n",
       "...               ...                    ...           ...   \n",
       "7608            False                     []          True   \n",
       "7609            False                     []         False   \n",
       "7610            False                     []          True   \n",
       "7611            False                     []         False   \n",
       "7612            False                     []          True   \n",
       "\n",
       "                         links  mention_in_text  ... word_count  \\\n",
       "0                           []            False  ...         13   \n",
       "1                           []            False  ...          8   \n",
       "2                           []            False  ...         25   \n",
       "3                           []            False  ...          8   \n",
       "4                           []            False  ...         16   \n",
       "...                        ...              ...  ...        ...   \n",
       "7608  [http://t.co/STfMbbZFB5]            False  ...         11   \n",
       "7609                        []             True  ...         22   \n",
       "7610  [http://t.co/zDtoyd8EbJ]            False  ...         15   \n",
       "7611                        []            False  ...         21   \n",
       "7612  [http://t.co/YmY4rSkQ3d]            False  ...         14   \n",
       "\n",
       "                                          unique_tokens unique_word_count  \\\n",
       "0     {of, Forgive, this, May, the, Deeds, Our, #ear...                13   \n",
       "1      {Sask, Canada, Forest, La, near, ., Ronge, fire}                 8   \n",
       "2     {expected, officers, ., to, ', in, residents, ...                20   \n",
       "3     {orders, receive, #wildfires, California, in, ...                 8   \n",
       "4     {school, as, sent, into, this, #wildfires, fro...                15   \n",
       "...                                                 ...               ...   \n",
       "7608  {giant, into, homes, collapse, Two, nearby, br...                11   \n",
       "7609  {of, state, even, troubling, fires, California...                18   \n",
       "7610  {Volcano, of, [, 5km, M1, UTC, ., 94, ], http:...                14   \n",
       "7611  {E-bike, injuries, ., in, an, with, collided, ...                20   \n",
       "7612  {-, Latest, News, Northern, Wildfire, Californ...                14   \n",
       "\n",
       "     stop_word_count  link_count hashtag_count  mention_count  char_count  \\\n",
       "0                  5           0             1              0          57   \n",
       "1                  0           0             0              0          32   \n",
       "2                  9           0             0              0         112   \n",
       "3                  1           0             1              0          57   \n",
       "4                  6           0             2              0          72   \n",
       "...              ...         ...           ...            ...         ...   \n",
       "7608               2           1             0              0          73   \n",
       "7609               7           0             0              2         106   \n",
       "7610               1           1             0              0          58   \n",
       "7611               5           0             0              0         119   \n",
       "7612               1           1             0              0          82   \n",
       "\n",
       "      mean_word_length  punctuation_count  \n",
       "0             4.384615                  0  \n",
       "1             4.000000                  1  \n",
       "2             4.480000                  3  \n",
       "3             7.125000                  0  \n",
       "4             4.500000                  0  \n",
       "...                ...                ...  \n",
       "7608          6.636364                  0  \n",
       "7609          4.818182                  2  \n",
       "7610          3.866667                  5  \n",
       "7611          5.666667                  2  \n",
       "7612          5.857143                  2  \n",
       "\n",
       "[7613 rows x 24 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.assign(\n",
    "    word_count=lambda x: x[\"tokens\"].apply(len),\n",
    "    unique_tokens=lambda x: x[\"tokens\"].apply(set),\n",
    "    unique_word_count=lambda x: x[\"unique_tokens\"].apply(len),\n",
    "    stop_word_count=lambda x: x[\"tokens\"].apply(\n",
    "        lambda y: len([token for token in y if token in STOPWORDS])\n",
    "    ),\n",
    "    link_count=lambda x: x[\"links\"].apply(len),\n",
    "    hashtag_count=lambda x: x[\"hashtags\"].apply(len),\n",
    "    mention_count=lambda x: x[\"mentions\"].apply(len),\n",
    "    char_count=lambda x: x[\"tokens\"].apply(lambda y: np.sum([len(token) for token in y])),\n",
    "    mean_word_length=lambda x: x[\"char_count\"] / x[\"word_count\"],\n",
    "    punctuation_count=lambda x: x[\"tokens\"].apply(\n",
    "        lambda y: len([token for token in y if token in string.punctuation])\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger, StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to take the sentence and output the lemmatized list\n",
    "    - Input is text\n",
    "    - Next tokenize and get pos_tag\n",
    "    - Use the pos_tage with token as input to the lemmatizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postag = StanfordPOSTagger('english-bidirectional-distsim.tagger',\n",
    "                           '/home/sjustice/My_code/stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "nert = StanfordNERTagger('/home/sjustice/My_code/stanford-tagger-4.0.0/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import download\n",
    "# download('averaged_perceptron_tagger')\n",
    "# download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are a lot of location categories that only appear once\n",
    "    - 2818 locations only appear once\n",
    "    - Some of them are in lowercase\n",
    "    - Convert the location to lower and remove punctuation\n",
    "    - Maybe remove those or ones with less than 5 occurances and replace them with None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replace all locations that appear only once with N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform a train/test split and then use the stemmer and the vectorizer on the train set\n",
    "- Need to be able to align the features from the train set so that they are the same in the test set\n",
    "    - Only include the ones in the train set - No new features in the test set\n",
    "    \n",
    "- Perform lematization on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[\n",
    "        [\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]\n",
    "    ],\n",
    "    train_df[\"target\"],\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"target\"],\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_list =['keyword', 'location', 'hash_in_text', 'handle_in_text']\n",
    "\n",
    "# Use a column transformer instead of the FeatureUnion since it accomplishes the same thing without\n",
    "# needing a custom class\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"text_transform\",\n",
    "            ColumnTransformer([(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat_feature_list),\n",
    "                                  (\"tfidf\", TfidfVectorizer(), 'lem_text')]),\n",
    "        ),\n",
    "#         (\"cudf_convert\", CudfConversion()),\n",
    "        (\"rf\", RandomForestClassifier(random_state=42, n_jobs=1) )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things to check for in the text\n",
    "    - @s - tweets directed at other users\n",
    "    - #s - hashtags\n",
    "    - retweets - urls\n",
    "    - Name entity\n",
    "    - Allcaps words??? - Remove them or keep them?\n",
    "    - Remove the ats, hashtags, and urls from the text before putting it through a tfidf vectorizer\n",
    "    - A lot of repeated location categories - Need to handle instances like 'Chicago, IL' and 'Chicago,IL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"rf__n_estimators\": range(3000, 10001, 500),\n",
    "    \"rf__max_depth\": [None, 10, 20, 40],\n",
    "    \"rf__min_samples_split\": [2, 4, 6],\n",
    "    \"rf__min_samples_leaf\": [1, 2, 3],\n",
    "    \"rf__max_features\": [\"auto\", \"log2\", 20, 40, 60, 100],\n",
    "    \"text_transform__tfidf__ngram_range\": [(1,1), (1,2), (1,3), (2,3)],\n",
    "    \"text_transform__tfidf__max_df\": [0.9, 0.95, 0.99],\n",
    "    \"text_transform__tfidf__min_df\": [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=30,\n",
    "                         scoring='f1', cv=3, random_state=42, verbose=2, n_jobs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  27 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=7)]: Done  90 out of  90 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('text_transform',\n",
       "                                              ColumnTransformer(transformers=[('ohe',\n",
       "                                                                               OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                               ['keyword',\n",
       "                                                                                'location',\n",
       "                                                                                'hash_in_text',\n",
       "                                                                                'handle_in_text']),\n",
       "                                                                              ('tfidf',\n",
       "                                                                               TfidfVectorizer(),\n",
       "                                                                               'lem_text')])),\n",
       "                                             ('rf',\n",
       "                                              RandomForestClassifier(n_jobs=1,\n",
       "                                                                     random_state=42))]),\n",
       "                   n_iter=30, n_jobs=7,\n",
       "                   param_distributions={'rf__max...0, 40],\n",
       "                                        'rf__max_features': ['auto', 'log2', 20,\n",
       "                                                             40, 60, 100],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 3],\n",
       "                                        'rf__min_samples_split': [2, 4, 6],\n",
       "                                        'rf__n_estimators': range(3000, 10001, 500),\n",
       "                                        'text_transform__tfidf__max_df': [0.9,\n",
       "                                                                          0.95,\n",
       "                                                                          0.99],\n",
       "                                        'text_transform__tfidf__min_df': [2, 5,\n",
       "                                                                          10],\n",
       "                                        'text_transform__tfidf__ngram_range': [(1,\n",
       "                                                                                1),\n",
       "                                                                               (1,\n",
       "                                                                                2),\n",
       "                                                                               (1,\n",
       "                                                                                3),\n",
       "                                                                               (2,\n",
       "                                                                                3)]},\n",
       "                   random_state=42, scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_transform__tfidf__ngram_range': (1, 1),\n",
       " 'text_transform__tfidf__min_df': 10,\n",
       " 'text_transform__tfidf__max_df': 0.99,\n",
       " 'rf__n_estimators': 7000,\n",
       " 'rf__min_samples_split': 4,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7210300429184551"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rf_model_with_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(rcv, f)\n",
    "\n",
    "# # and later you can load it\n",
    "# with open('filename.pkl', 'rb') as f:\n",
    "#     clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (pd.read_csv(\"./test.csv\")\n",
    "           .pipe(add_features, tokenizer=tknzr, lemmatizer=wnl)\n",
    "           .pipe(reduce_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>handle_in_text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6234</th>\n",
       "      <td>snowstorm</td>\n",
       "      <td>south usa</td>\n",
       "      <td>Sassy city girl country hunk stranded in Smoky...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Sassy city girl country hunk strand in Smoky M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>armageddon</td>\n",
       "      <td>worldwide</td>\n",
       "      <td>God's Kingdom (Heavenly Gov't) will rule over ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>God's Kingdom ( Heavenly Gov't ) will rule ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>body%20bagging</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Mopheme and Bigstar Johnson are a problem in t...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Mopheme and Bigstar Johnson be a problem in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>whirlwind</td>\n",
       "      <td>N/A</td>\n",
       "      <td>@VixMeldrew sounds like a whirlwind life!</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@VixMeldrew sound like a whirlwind life !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>debris</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>Malaysia confirms plane debris washed up on Re...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Malaysia confirm plane debris wash up on Reuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             keyword   location  \\\n",
       "6234       snowstorm  south usa   \n",
       "326       armageddon  worldwide   \n",
       "997   body%20bagging        N/A   \n",
       "7269       whirlwind        N/A   \n",
       "2189          debris    nigeria   \n",
       "\n",
       "                                                   text  hashtag_in_text  \\\n",
       "6234  Sassy city girl country hunk stranded in Smoky...             True   \n",
       "326   God's Kingdom (Heavenly Gov't) will rule over ...            False   \n",
       "997   Mopheme and Bigstar Johnson are a problem in t...             True   \n",
       "7269          @VixMeldrew sounds like a whirlwind life!            False   \n",
       "2189  Malaysia confirms plane debris washed up on Re...            False   \n",
       "\n",
       "      link_in_text  handle_in_text  \\\n",
       "6234          True           False   \n",
       "326           True           False   \n",
       "997          False           False   \n",
       "7269         False            True   \n",
       "2189          True           False   \n",
       "\n",
       "                                               lem_text  \n",
       "6234  Sassy city girl country hunk strand in Smoky M...  \n",
       "326   God's Kingdom ( Heavenly Gov't ) will rule ove...  \n",
       "997   Mopheme and Bigstar Johnson be a problem in th...  \n",
       "7269          @VixMeldrew sound like a whirlwind life !  \n",
       "2189  Malaysia confirm plane debris wash up on Reuni...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_in_text</th>\n",
       "      <th>link_in_text</th>\n",
       "      <th>handle_in_text</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Just happen a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Heard about #earthquake be different city , st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>there be a forest fire at spot pond , geese be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Apocalypse light . #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Typhoon Soudelor kill 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  \\\n",
       "0     N/A      N/A                 Just happened a terrible car crash   \n",
       "1     N/A      N/A  Heard about #earthquake is different cities, s...   \n",
       "2     N/A      N/A  there is a forest fire at spot pond, geese are...   \n",
       "3     N/A      N/A           Apocalypse lighting. #Spokane #wildfires   \n",
       "4     N/A      N/A      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   hashtag_in_text  link_in_text  handle_in_text  \\\n",
       "0            False         False           False   \n",
       "1             True         False           False   \n",
       "2            False         False           False   \n",
       "3             True         False           False   \n",
       "4            False         False           False   \n",
       "\n",
       "                                            lem_text  \n",
       "0                   Just happen a terrible car crash  \n",
       "1  Heard about #earthquake be different city , st...  \n",
       "2  there be a forest fire at spot pond , geese be...  \n",
       "3             Apocalypse light . #Spokane #wildfires  \n",
       "4       Typhoon Soudelor kill 28 in China and Taiwan  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe.predict(test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].rename(columns={\"hashtag_in_text\": \"hash_in_text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(target=best_pipe.predict(test_df[[\n",
    "            \"keyword\",\n",
    "            \"location\",\n",
    "            \"text\",\n",
    "            \"hashtag_in_text\",\n",
    "            \"link_in_text\",\n",
    "            \"handle_in_text\",\n",
    "#             \"tokens\",\n",
    "#             \"lem_tokens\",\n",
    "            \"lem_text\",\n",
    "        ]].rename(columns={\"hashtag_in_text\": \"hash_in_text\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id', 'target']].to_csv(\"submission_0706.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
